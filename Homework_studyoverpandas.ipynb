{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-19T17:33:34.797677Z","iopub.execute_input":"2021-11-19T17:33:34.798467Z","iopub.status.idle":"2021-11-19T17:33:35.784287Z","shell.execute_reply.started":"2021-11-19T17:33:34.798418Z","shell.execute_reply":"2021-11-19T17:33:35.783413Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# 1) Read the dataset as a dataframe. \n# Create a copy of your dataframe. \n# Solve the rest of the questions using this dataframe copy.\n\ndf=pd.read_csv(\"/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:33:35.786231Z","iopub.execute_input":"2021-11-19T17:33:35.786508Z","iopub.status.idle":"2021-11-19T17:33:36.417370Z","shell.execute_reply.started":"2021-11-19T17:33:35.786467Z","shell.execute_reply":"2021-11-19T17:33:36.416504Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.head(1).T","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:33:36.418570Z","iopub.execute_input":"2021-11-19T17:33:36.418809Z","iopub.status.idle":"2021-11-19T17:33:36.441578Z","shell.execute_reply.started":"2021-11-19T17:33:36.418781Z","shell.execute_reply":"2021-11-19T17:33:36.440699Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# 2) Find out the describe and info attributes of the dataframe. \n# Analyze these information and create a short write-up according to your findings.\n\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:33:36.442591Z","iopub.execute_input":"2021-11-19T17:33:36.442794Z","iopub.status.idle":"2021-11-19T17:33:36.584605Z","shell.execute_reply.started":"2021-11-19T17:33:36.442768Z","shell.execute_reply":"2021-11-19T17:33:36.583676Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"    df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:33:36.587582Z","iopub.execute_input":"2021-11-19T17:33:36.587829Z","iopub.status.idle":"2021-11-19T17:33:36.724217Z","shell.execute_reply.started":"2021-11-19T17:33:36.587799Z","shell.execute_reply":"2021-11-19T17:33:36.723350Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# 3) Find out the shape and size info of the dataset.\n\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:33:36.725689Z","iopub.execute_input":"2021-11-19T17:33:36.725979Z","iopub.status.idle":"2021-11-19T17:33:36.732522Z","shell.execute_reply.started":"2021-11-19T17:33:36.725940Z","shell.execute_reply":"2021-11-19T17:33:36.731616Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.size","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:33:36.733914Z","iopub.execute_input":"2021-11-19T17:33:36.734169Z","iopub.status.idle":"2021-11-19T17:33:36.744526Z","shell.execute_reply.started":"2021-11-19T17:33:36.734131Z","shell.execute_reply":"2021-11-19T17:33:36.743629Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# 4) Find out the types values of the columns and save the result as a dataframe.,\ndf_dtype=df.dtypes\ndf_dtype","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:33:36.745564Z","iopub.execute_input":"2021-11-19T17:33:36.745777Z","iopub.status.idle":"2021-11-19T17:33:36.758045Z","shell.execute_reply.started":"2021-11-19T17:33:36.745750Z","shell.execute_reply":"2021-11-19T17:33:36.757072Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# 5) Find out the non-null counts of the columns and save the result as a dataframe.\n\ndf_nonNul=df.dropna()\ndf_nonNul_count=df.notnull().sum()\n\ndf_nonNul_count\n#df.dropna().count()[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:33:36.758971Z","iopub.execute_input":"2021-11-19T17:33:36.759222Z","iopub.status.idle":"2021-11-19T17:33:37.016025Z","shell.execute_reply.started":"2021-11-19T17:33:36.759195Z","shell.execute_reply":"2021-11-19T17:33:37.015207Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# 6) Find out the null counts of the columns and save the result as a dataframe.\n\ndf_nul_count=df.isnull().sum()\ndf_nul_count","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:33:37.017485Z","iopub.execute_input":"2021-11-19T17:33:37.017724Z","iopub.status.idle":"2021-11-19T17:33:37.139853Z","shell.execute_reply.started":"2021-11-19T17:33:37.017693Z","shell.execute_reply":"2021-11-19T17:33:37.139013Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# 7) Find out the unique counts of the columns and save the result as a dataframe.\n\n#df['Date'].value_counts().count()    # Bu şekilde yazdığımda her kolon için unique değeri ayrı ayrı bulabiliyorum.\n#df.value_counts().count()            # Ancak bu şekilde yazdığımda tüm kolonlar çıkmıyor. Bu yöntem is nul veya not nul da işe yaramıştı\n\ndf_unique_count=df.apply(pd.value_counts).count()   # Sorunu çözmek için bunu denedim oldu.\ndf_unique_count","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:33:37.141344Z","iopub.execute_input":"2021-11-19T17:33:37.141571Z","iopub.status.idle":"2021-11-19T17:33:37.483832Z","shell.execute_reply.started":"2021-11-19T17:33:37.141542Z","shell.execute_reply":"2021-11-19T17:33:37.482986Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# 8) Merge the dataframes you created in questions 4-5-6-7. Expected output:\n\nlst = [df_dtype,df_nonNul_count,df_nul_count,df_unique_count]\nmerged_df = pd.concat(lst,axis=1)\nmerged_df.columns=[ \"dtype\", \"notnull\" ,\"isnull\", \"unique\"]\nmerged_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:33:37.485142Z","iopub.execute_input":"2021-11-19T17:33:37.485451Z","iopub.status.idle":"2021-11-19T17:33:37.499758Z","shell.execute_reply.started":"2021-11-19T17:33:37.485418Z","shell.execute_reply":"2021-11-19T17:33:37.498558Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# 9) Lowercase all column names.\n\ndf.columns= df.columns.str.lower()\ndf.head().T","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:36:21.409245Z","iopub.execute_input":"2021-11-19T17:36:21.410145Z","iopub.status.idle":"2021-11-19T17:36:21.426535Z","shell.execute_reply.started":"2021-11-19T17:36:21.410083Z","shell.execute_reply":"2021-11-19T17:36:21.425359Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# 10) Change all the No values to NoRain and all the Yes values to Rain in raintoday and raintomorrow columns.\n\ndf.raintoday.replace(\"No\",\"NoRain\", inplace=True)\ndf.raintoday.replace(\"Yes\",\"Rain\", inplace=True)\ndf.raintomorrow.replace(\"No\",\"NoRain\", inplace=True)\ndf.raintomorrow.replace(\"Yes\",\"Rain\", inplace=True)\n\nprint(df[[\"raintoday\",\"raintomorrow\"]])\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:53:35.551094Z","iopub.execute_input":"2021-11-19T17:53:35.551438Z","iopub.status.idle":"2021-11-19T17:53:35.591334Z","shell.execute_reply.started":"2021-11-19T17:53:35.551405Z","shell.execute_reply":"2021-11-19T17:53:35.590259Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# 11) Change the data type of \"date\" (object) column to datetime64 and reformat the date as DD/MM/YYYY.\n\ndf['date'].dtype\n\ndf['date'] = pd.to_datetime(df['date'], errors='coerce')\ndf['date'] = df['date'].dt.strftime('%d/%m/%Y')\ndf['date']\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:05:34.653302Z","iopub.execute_input":"2021-11-19T18:05:34.653936Z","iopub.status.idle":"2021-11-19T18:05:35.728980Z","shell.execute_reply.started":"2021-11-19T18:05:34.653884Z","shell.execute_reply":"2021-11-19T18:05:35.728171Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# 12) Create a new column called \"difference\", calculate the difference between maxtemp and mintemp columns for each row, \n# and store the value in this new column.\n\ndf['difference'] = df['maxtemp'] - df['mintemp']\ndf.head().T\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:08:16.185008Z","iopub.execute_input":"2021-11-19T18:08:16.185574Z","iopub.status.idle":"2021-11-19T18:08:16.207798Z","shell.execute_reply.started":"2021-11-19T18:08:16.185497Z","shell.execute_reply":"2021-11-19T18:08:16.206911Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# 13) Remove the evaporation and sunshine columns from the dataset permanently.\n\n \ndf.drop(['evaporation','sunshine'],axis = 1, inplace = False).head().T\n\n#df.drop(['evaporation','sunshine'],axis = 1, inplace = True).head().T","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:12:54.586814Z","iopub.execute_input":"2021-11-19T18:12:54.587159Z","iopub.status.idle":"2021-11-19T18:12:54.623217Z","shell.execute_reply.started":"2021-11-19T18:12:54.587109Z","shell.execute_reply":"2021-11-19T18:12:54.622564Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# 14) Find out the most rainy day for each city.\n\ndf.groupby(\"location\").aggregate({\"rainfall\":\"max\"})","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:20:26.039349Z","iopub.execute_input":"2021-11-19T18:20:26.039645Z","iopub.status.idle":"2021-11-19T18:20:26.074497Z","shell.execute_reply.started":"2021-11-19T18:20:26.039605Z","shell.execute_reply":"2021-11-19T18:20:26.073187Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# 15) Filter out all the data for the city 'Albury' and then sort according to maxtemp column.\n\ndf_Albury = df[df.location == 'Albury']\ndf_Albury.head().T\ndf_Albury.sort_values(by=['maxtemp']).T\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:26:55.250204Z","iopub.execute_input":"2021-11-19T18:26:55.250509Z","iopub.status.idle":"2021-11-19T18:26:55.404092Z","shell.execute_reply.started":"2021-11-19T18:26:55.250476Z","shell.execute_reply":"2021-11-19T18:26:55.402997Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# 16) Find out the NaN counts for each column.\n\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:29:52.695168Z","iopub.execute_input":"2021-11-19T18:29:52.695528Z","iopub.status.idle":"2021-11-19T18:29:52.957974Z","shell.execute_reply.started":"2021-11-19T18:29:52.695486Z","shell.execute_reply":"2021-11-19T18:29:52.957149Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# 17) Remove the rows with NaN values in \"windgustdir\" column from the dataframe permanently.\n\ndf.dropna(subset=['windgustdir'],inplace = False).T\n\n#df.dropna(subset=['windgustdir'],inplace = True)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:31:41.553318Z","iopub.execute_input":"2021-11-19T18:31:41.554395Z","iopub.status.idle":"2021-11-19T18:31:46.672314Z","shell.execute_reply.started":"2021-11-19T18:31:41.554330Z","shell.execute_reply":"2021-11-19T18:31:46.671453Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# 18) Create a new dataframe, use \"Location\" column as the index of the dataframe, \n# display the min, max, and median values of \"evaporation\" and \"sunshine\" columns in this dataframe.\n\n\ndf.groupby('location').aggregate({'evaporation':['min','max',np.median],'sunshine':['min','max',np.median]})\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:35:15.058928Z","iopub.execute_input":"2021-11-19T18:35:15.059226Z","iopub.status.idle":"2021-11-19T18:35:15.126907Z","shell.execute_reply.started":"2021-11-19T18:35:15.059194Z","shell.execute_reply":"2021-11-19T18:35:15.126213Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# 19) Find out the hottest day of \"Perth\". Example output: Timestamp('2015-01-05 00:00:00')\n\ndf[(df.location=='Perth') & df.maxtemp==True].sort_values(by=['maxtemp']).tail() # ikinci koşul nan olan değerleri almaması için\n\ndf[(df.location=='Perth') & df.maxtemp==True].sort_values(by=['maxtemp']).tail(1)['date']\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:43:44.265733Z","iopub.execute_input":"2021-11-19T18:43:44.266051Z","iopub.status.idle":"2021-11-19T18:43:44.343564Z","shell.execute_reply.started":"2021-11-19T18:43:44.266010Z","shell.execute_reply":"2021-11-19T18:43:44.342687Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# 20) Group your dataframe by location and find out the averages of all numeric values.\n\ndf.groupby('location').mean()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T18:46:10.215065Z","iopub.execute_input":"2021-11-19T18:46:10.215408Z","iopub.status.idle":"2021-11-19T18:46:10.303190Z","shell.execute_reply.started":"2021-11-19T18:46:10.215373Z","shell.execute_reply":"2021-11-19T18:46:10.302206Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}